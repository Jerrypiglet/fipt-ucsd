<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="FIPT, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIPT</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/OIG.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
  <!-- <script>
		bulmaCarousel.attach('#carousel-demo', {
			slidesToScroll: 1,
			slidesToShow: 2
		});
		</script> -->
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FIPT: Factorized Inverse Path Tracing <br> for Efficient and Accurate Material-Lighting Estimation</h1>

          <div class="is-size-4">
            <span class="publication-venue"><b>ICCV 2023 <span style="color:blue">(oral presentation)</span></b></span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top:15px;">
            <span class="author-block">
              <a href="https://lwwu2.github.io/">Liwen Wu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jerrypiglet.github.io/">Rui Zhu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://myaldiz.info/">Mustafa B. Yaldiz</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yinhaoz.github.io/">Yinhao Zhu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://herbertcai.github.io/">Hong Cai</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://janarbek.github.io/">Janarbek Matai</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.porikli.com/">Fatih Porikli</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://cseweb.ucsd.edu/~tzli/">Tzu-Mao Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup><b>equal contribution</b></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California San Diego,</span>
            <span class="author-block"><sup>2</sup>Qualcomm AI Research</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- <a href="./static/FIPT_arxiv.pdf" -->
                  <a href="./static/05648-combined.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.05669"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (coming soon)</span>
                </a>
              </span>
              <!-- Slides Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-slideshare"></i>
                  </span>
                  <span>Slides (coming soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lwwu2/fipt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1N8H1yR41MykUuSTyHvKGsZcuV2VjtWGr?usp=share_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data download</span>
                  </a> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/Jerrypiglet/rui-indoorinv-data/tree/fipt"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <img src="./static/images/teaser.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>

      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Factorized Inverse Path Tracing (FIPT)</span> reduces ambiguity and Monte Carlo variance in inverse rendering,
        yielding efficient and high quality BRDF-emission, 
        appealing relighting, and object insertion results. (See Related Works section for baseline methods.)
      </h2>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fipt_demo_1_H264.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fipt_demo_2_H264.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/placeholder.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/placeholder.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Inverse path tracing has recently been applied to joint material and lighting estimation, given geometry and multi-view HDR observations of an indoor scene.
            However, it has two major limitations:
            path tracing is expensive to compute,
            and 
            ambiguities exist between reflection and emission.
          </p>
          <p>
            We propose a novel <span class="dnerf">Factorized Inverse Path Tracing (FIPT)</span> method
            which utilizes a factored light transport formulation 
            and finds emitters driven by rendering errors. 
          </p>
          <p>
            Our algorithm enables accurate material and lighting optimization faster than previous work, and is more effective at resolving ambiguities.
            The exhaustive experiments on synthetic scenes show that our method 
            (1) outperforms state-of-the-art indoor inverse rendering and relighting methods particularly in the presence of complex illumination effects; 
            (2) speeds up inverse path tracing optimization 
            to less than an hour.  
            We further demonstrate robustness to noisy inputs through material and lighting estimates that allow plausible relighting in a real scene.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    
    <!-- <section class="hero teaser">
      <div class="hero-body">
          <div class="container is-max-desktop"> -->
    <!--/ Paper video. -->
  </div>
</section>

<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- <div class="column"> -->
      <div class="column is-three-fifths has-text-centered">
          <div class="content">
          <h2 class="title is-3">Pipeline</h2>
          <!-- <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p> -->
          <img src="./static/images/pipeline.drawio-cropped.svg"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-4">Shading initialization</h2>
          <p>
            Diffuse and specular shadings are <b>initialized</b> by
            tracing a <b>voxel representation of the surface light field</b> <b>L'</b>
            (left), which gives approximations (top row on right) close
            to the ground truth (bottom row on the right; obtained from a physical path tracer).
          </p>
          <img src="./static/images/shading_1.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
        </div>
      </div>
      
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Shading refinement</h2>
          <p>
            The wall cabinet's diffuse reflectance
              estimation is initially darker than ground truth, owing to
              the excessive incident light received from the range hood
              that reflects non-diffuse light (2nd column). The artifacts
              are reduced by <b>growing the path</b> for the specular surface
              according to the optimized BRDF (1st column), which gives
              more accurate shadings that can be used to further refine the
              BRDF (3rd column).
          </p>
          <img src="./static/images/shading_2.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <h2 class="title is-4">Error-driven emitter estimation</h2>
          <p>
            Optimization without emission terms produces distinctive error near emissive surfaces (1st column). 
            By jointly <b>optimizing an emission mask</b> to cancel this error, the <b>emitter</b> can be found by checking the mask's response (works even for tiny emitters), 
            and its emission value can be obtained by median-pooling the RGBs from training pixels.
          </p>
          <img src="./static/images/emission.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
        </div>
      </div>


    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Synthetic scenes</h2>
    <p>
      <b>Qualitative comparison of BRDF and emission on synthetic scenes</b> shows our method successfully reconstructs material reflectance (1st row), roughness (2nd row), and emission (3rd row) with high frequency details and less ambiguity. Emission estimation is shown as error heatmaps (warmer colors indicate higher emission error; GT emitter boundary is marked in white lines).
    </p>
    <img src="./static/images/synthetic_results_BRDF.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
    <img src="./static/images/synthetic_results_render.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Real world captures and results</h2>
    <p>
      <b>The capture setting (left) and observations of the real world scenes (middle and right).</b>
      We present two real world scenes <b>Conference room</b> and <b>Classroom</b> with samples of captured images, reconstructed geometries in 3 views, and all camera poses.
    </p>
    <img src="./static/images/real.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
  </div>
  
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-4">BRDF and emission estimation results</h2>
      <p>
        Showing 2 views per-scene. Top row shows the input views.
      </p>
      <img src="./static/images/real_results_BRDF.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
        
    </div>
</div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-4">Rerendering and relighting results</h2>
      <p>
        Showing 2 views per-scene for each task. Top row shows the
        input images with original lighting (Conference room: all ceiling lamps on; Classroom: rear lights on and fronts lights off).
        Reference photos for relit Classroom are also included as pseudo-ground truth (with rear lights off, and front lights on).
      </p>
      <img src="./static/images/real_results_render.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
    </div>
</div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>

        <div class="content has-text-justified">
          <p>
            There are numerous excellent works that are most closely related to ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/1903.07145"><b>IPT</b></a> and <a href="https://ci.idm.pku.edu.cn/Yu_TPAMI23.pdf"><b>MILO</b></a> both utilize differentiable path tracing to jointly optimize dense scene BRDF and emission. 
          </p>
          
          <p>
            IPT takes a piecewise constant parameterization of material to reduce Monte Carlo variance and ambiguity for inverse rendering, losing fine spatial details as a result (similarly in <a href="https://rgl.epfl.ch/publications/NimierDavid2021Material"><b>Nimier-David et al.</b></a>). 
            Directly extending it to complex material representation (i.e. MILO) shows very slow convergence.
          </p>

          <p>
            In a parallel line of work, NeRF-like methods have been representating incident radiance field of indoor scenes as a 5D network (e.g. <a href="https://machinelearning.apple.com/research/neural-incident-light-field"><b>NeILF</b></a>), usually in unconstrained fashion. 
            Most recently, numerous works adopt similar ideas to ours by using pre-baked irradiance (e.g. 
            <a href="https://arxiv.org/abs/2211.10206"><b>TexIR</b></a>, 
            <a href="https://jingsenzhu.github.io/i2-sdf/"><b>I<sup>2</sup>-SDF</b></a>, 
            <a href="https://yoyo000.github.io/NeILF_pp/"><b>NeILF++</b></a>, 
            <a href="https://arxiv.org/abs/2303.16617"><b>NeFII</b></a>), 
            and resort to surface rendering to recover scene materials and/or emission, without modeling global light transport.
          </p>

          <p>
            In contrast to the aforementioned optimization-based approaches, learning-based approaches leverage lighting and material priors learned from datasets.
            <a href="https://repo-sam.inria.fr/fungraph/deep-indoor-relight"><b>Philip et al. (FVP)</b></a> take multiple images and aggregate multiview irradiance and albedo information to a pre-trained network to synthesize the relit image.
            <a href="https://vilab-ucsd.github.io/ucsd-IndoorLightEditing/"><b>Li et al. (Li22)</b></a>, <a href="https://vilab-ucsd.github.io/ucsd-irisformer/"><b>Zhu et al. (IRISFormer)</b></a>, 
            <a href="https://jingsenzhu.github.io/invrend/"><b>Zhu et al.</b></a>, and more recently, 
            <a href="https://bring728.github.io/mair.project/"><b>Choi et al. (MAIR)</b></a>, learn single-view or multi-view dense lighting and/or BRDF from shading cues or scene features by training on large-scale synthetic datasets.
          </p>

          <p>
            There is a growing list of related papers by the time you read this. Please feel welcomed to <a href="https://github.com/Jerrypiglet/fipt-ucsd/discussions">let us know</a> if we missed anything and we will update here.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{fipt2023,
    title={Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation}, 
    author={Liwen Wu and Rui Zhu and Mustafa B. Yaldiz and Yinhao Zhu and Hong Cai and Janarbek Matai and Fatih Porikli and Tzu-Mao Li and Manmohan Chandraker and Ravi Ramamoorthi},
    year={2023},
    eprint={2304.05669},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website. -->
            <p>
              Credit: this website is adapted from the amazing template of <a href="https://github.com/nerfies/nerfies.github.io">nerfies.</a>
            </p>
            
            <p>
              Easter egg: the <a href="./static/images/OIG.jpg">favicon</a> on the tab of the page was created with <a href="https://www.bing.com/images/create/standford-bunny-in-metallic-blue-color-on-a-dining/6435149f35d948198b3d180620a99e26?FORM=GENCRE">everyone's favorite renderer nowadays</a>. 
              Shout out loud if you think generative models for indoor scene editing are the future. <a href="https://github.com/Jerrypiglet/fipt-ucsd/tree/main/static/images/more_bunnies">[Gimme more 🐇]</a>
            </p>

          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
